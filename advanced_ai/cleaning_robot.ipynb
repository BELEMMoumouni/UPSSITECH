{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FRjP0bPme0t",
        "outputId": "81195332-827d-4f82-8414-2c6063f88172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymdptoolbox\n",
            "  Downloading pymdptoolbox-4.0-b3.zip (29 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pymdptoolbox) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pymdptoolbox) (1.7.3)\n",
            "Building wheels for collected packages: pymdptoolbox\n",
            "  Building wheel for pymdptoolbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymdptoolbox: filename=pymdptoolbox-4.0b3-py3-none-any.whl size=25657 sha256=b664e675e2db3c33a47d2c8a1d7d5b41d55fefd34b338720b7ac9b99e2aa992e\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/a8/27/a76d688633fa5d71984c288499c2170a8d06726135b8e216fd\n",
            "Successfully built pymdptoolbox\n",
            "Installing collected packages: pymdptoolbox\n",
            "Successfully installed pymdptoolbox-4.0b3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install pymdptoolbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "W32Rgm40me0v"
      },
      "outputs": [],
      "source": [
        "import mdptoolbox, mdptoolbox.example, mdptoolbox.util\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": false,
        "id": "V6r9-onome0w"
      },
      "source": [
        "# The Cleaning Robot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MHcvRI4me0x"
      },
      "source": [
        "Dead line : 11 nov for both groups (note book + report describing your mdp, including a drawing)\n",
        "\n",
        "Consider a house-cleaning robot. It can be either in the living room or at its charging station - or out  of battery. \n",
        "The living room can be clean or dirty. So there are five states: LD(in the living room, dirty), LC(in the living room, clean), CD(at the charger, dirty), CC(at the charger, clean), O (out of power).\n",
        "\n",
        "\n",
        "####  \n",
        "When in the living room,    the robot  can either choose to clean or to charge. \n",
        "\n",
        "When in the charging station, the robot can either choose to clean or to keep charging.\n",
        "\n",
        "When out of order, any of the two actions (clean, charge) leads to the same results: staying out of power\n",
        "\n",
        "####  \n",
        "\n",
        "The reward for being  in a clean state is rc \n",
        "\n",
        "The reward for being in a dirty state is rd < rc at each time step\n",
        "\n",
        "The reward for being out of power is  costcrash  -  lower or equal to rd ; let us set it equal to rd  (the living rooms becomes dirty anyway)\n",
        " \n",
        "\n",
        "####  \n",
        "\n",
        "When  the robot decides to  clean,\n",
        "*  if it is in the living room, then it may become out of power with proba e\n",
        "*  if it is in the charging station, no risk of running out of power   \n",
        "*  cleaning a clean floor leaves it clean\n",
        "*  cleaning a dirty floor can sometimes fail (even is battery is ok) - let eps be the probability of fail of the cleaning\n",
        "     \n",
        "When  the robot decides to  charge,  action charge always takes the robot to the charging station. The  dirtiness of the room can go from clean to dirty with a probability  pDust  and it stays for sure at the dirty level if dirty\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1dSKpsRme0x"
      },
      "source": [
        "\n",
        "#### Goal\n",
        "\n",
        "Model the problem by a (infinite horizon) MDP (describe your model in details) \n",
        "\n",
        "As joined the draw explaining the mdp, you could differents states and links them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "collapsed": true,
        "id": "28vjCAmIme0y"
      },
      "outputs": [],
      "source": [
        "def matrice_transitions_reward(rc, rd, costcrash, e, eps, pDust):\n",
        "    R =np.array(\n",
        "        [[rd, rd],\n",
        "        [rc, rc],\n",
        "        [rd, rd],\n",
        "        [rc, rc],\n",
        "        [costcrash, costcrash]])\n",
        "    \n",
        "\n",
        "        #CLEAN     \n",
        "    P = np.array(\n",
        "         #LD  #LC  #CD  #CC  #O\n",
        "        [[[eps,1-eps-e,0.,0.,e], #LD\n",
        "        [0,1-e,0.,0.,e],         #LC\n",
        "        [eps,1-eps,0.,0.,0.],    #CD\n",
        "        [0.,1,0.,0.,0.],         #CC\n",
        "        [0.,0.,0.,0.,1]],        #O\n",
        "        \n",
        "        #CHARGE\n",
        "         #LD  #LC  #CD  #CC  #O\n",
        "        [[ 0., 0., 1, 0, 0.],       #LD\n",
        "         [ 0., 0., 0., 1, 0],       #LC\n",
        "         [ 0., 0., 1, 0., 0.],       #CD\n",
        "         [ 0., 0., pDust,1-pDust,0.],#CC\n",
        "         [ 0., 0., 0., 0., 1]]      #O\n",
        "        ])    \n",
        "        \n",
        "        \n",
        "    return P, R\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0Tg-vh1me0y",
        "outputId": "ad985e6f-2eb1-4ba2-c8ef-7cdc08aee4d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 1, 1, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ],
      "source": [
        "rd= -10\n",
        "rc =10\n",
        "costcrash = 10\n",
        "e = 1\n",
        "eps = 0\n",
        "pDust = 0.2\n",
        "\n",
        "P, R = matrice_transitions_reward(rc, rd, costcrash, e, eps, pDust)\n",
        "\n",
        "finiteHorizon = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
        "#finiteHorizon = mdptoolbox.mdp.FiniteHorizon(P,R,0.9,1)\n",
        "finiteHorizon.run()\n",
        "finiteHorizon.setVerbose()\n",
        "#finiteHorizon.policy[:,0]\n",
        "finiteHorizon.policy\n",
        "#LD LC CD CC O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1dwHXWYme0z"
      },
      "source": [
        "Determine the best policy.\n",
        "\n",
        "* When the probability of runing out of battery is high\n",
        "> For e = 0.9 , we got this policy: (0, 0, 0, 0, 0)\n",
        "\n",
        "\n",
        "\n",
        "* When it is low\n",
        ">   For e = 0.1, we got (0, 0, 0, 0, 0) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "(explain the results)\n",
        "<br> \n",
        "\n",
        "> We just have the same result even changing the probability of out  of power in all states and the robot chooses to clean.\n",
        "Results can be explained by hte fact that in any of the 5 states, choosing to clean leads to the same reward : either in clean state or in O. In both, the reward is same and greater.\n",
        "It could fail in the first state LD by it has a chance to get a great reward by cleaning than charging( charging leads to a dirty state ) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f-iqvQIJme0z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_iGIHECme00"
      },
      "source": [
        "* What policy is choosen if  the robot has a very good battery (i.e. the probability of being out  of charge is very low) ? \n",
        "\n",
        "\n",
        ">         When the   robot has a very good battery ( I set e = 0.1), we got this policy : (0, 0, 0, 0, 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* What policy is choosen if the  living room gets dirty   quickly  (i.e.  pdust increases) ? \n",
        ">  When pDust increases  ( pDust = 0.8 , 1 ), we got this policy : (0, 0, 0, 0, 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* When e=0.1 does the best policy depend of rc ? of pDust?  \n",
        "\n",
        "    When e = 0.1, tests show that policy depends on rc at some level of rc rising. But not really of pDust\n",
        "\n",
        "\n",
        "* What if costcrash  is independent of rc ?\n",
        "> if costcrash is independent of rc, we could face 3 case:\n",
        " <br> * if rc > costcrash and this case in LC, the action is going to be chosen beacause it is well rewarded to go a clean state than in dirty one.\n",
        " <br> * rc < costcrash : In this case, cleaning is going to be chosen favouritely."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste your code"
      ],
      "metadata": {
        "id": "Rpz54vBvQNJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bestPolicy(batterieProba):\n",
        "  e = batterieProba\n",
        "  finiteHorizon = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
        "  finiteHorizon.run()\n",
        "  finiteHorizon.setVerbose()\n",
        "  print(finiteHorizon.policy)\n",
        "\n",
        "\n",
        "\n",
        "bestPolicy(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRazIuqqQLhp",
        "outputId": "5df478ba-a640-4fa2-d9b5-91dc62607c5a"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0, 0, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hiSwUhZNme01"
      },
      "source": [
        "\n",
        "Optional: extend your program so as to take into consideration several levels of battery (high, medium, low, very low for instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntHZfQEIme01",
        "outputId": "a1be8ee3-f45c-4c34-cdeb-85c34e4970eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0, 0, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "batteries = [\"high\", \"medium\", \"low\", \"very low\"]\n",
        "\n",
        "probaBatteries = [0.9, .05, 0.3, 0.1]\n",
        "\n",
        "def baterieOption(batterielevel):\n",
        "    if batterielevel == \"high\":\n",
        "      e = probaBatteries[0]\n",
        "    if batterielevel == \"medium\":\n",
        "      e = probaBatteries[1]\n",
        "    if batterielevel == \"low\":\n",
        "      e = probaBatteries[2]\n",
        "    if batterielevel == \"very low\":\n",
        "      e = probaBatteries[3]\n",
        "    bestPolicy(e)\n",
        "\n",
        "\n",
        "baterieOption(\"high\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ee33b22a4495e93d58c602f85d93855cfb507df08ca7f946296387088ba3047b"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
